{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snow_nonts_analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCGOsNC1F6EV"
      },
      "source": [
        "# Description\n",
        "This will focus on using classic regression models and bayesian models. \n",
        "\n",
        "#### NOTE: As described in EDA notebook, \"Pseudo_ts\" is concatenation of data from locally adjacent ski resorts (e.g., all resorts in Colorado) into a single timeseries.\n",
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hkRrNSaGDae",
        "outputId": "8b9d228f-50f9-41cb-80b8-b2c7b8617742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "! pip install vapeplot arviz pystan stan_utility"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vapeplot in /usr/local/lib/python3.6/dist-packages (0.0.8)\n",
            "Requirement already satisfied: arviz in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pystan in /usr/local/lib/python3.6/dist-packages (2.19.1.1)\n",
            "Requirement already satisfied: stan_utility in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from vapeplot) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from vapeplot) (3.2.2)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from arviz) (0.16.1)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.6/dist-packages (from arviz) (50.3.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from arviz) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from arviz) (20.4)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from arviz) (1.4.1)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.6/dist-packages (from arviz) (1.5.4)\n",
            "Requirement already satisfied: Cython!=0.25.1,>=0.22 in /usr/local/lib/python3.6/dist-packages (from pystan) (0.29.21)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from stan_utility) (2.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->arviz) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->arviz) (1.15.0)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.6/dist-packages (from netcdf4->arviz) (1.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNlQEkolNSWR",
        "outputId": "d9fe4b54-e5d8-4f78-8775-7e79e46894ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    base_path = r'/content/gdrive/My Drive/data_sci/colab/ski/'\n",
        "    os.chdir(base_path)\n",
        "    try:\n",
        "        ! git clone https://github.com/chrisoyer/ski-snow-modeling/\n",
        "    except:  # if dir not empty e.g. already cloned\n",
        "        ! git pull\n",
        "    mod_path = os.path.join(base_path, \n",
        "                            r\"ski-snow-modeling/src/analysis/project_utils/project_utils.py\")\n",
        "    import importlib.util\n",
        "    spec = importlib.util.spec_from_file_location(name=\"utils.name\", location=mod_path)\n",
        "    utils = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(utils)\n",
        "    \n",
        "    os.chdir('./ski-snow-modeling/src/analysis/')\n",
        "    # Change the working directory to the repo root.\n",
        "    # Add the repo root to the Python path.\n",
        "    import sys\n",
        "    sys.path.append(os.getcwd())\n",
        "else:\n",
        "    # local running\n",
        "    import project_utils as utils"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "fatal: destination path 'ski-snow-modeling' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7TktoEhF6EX",
        "outputId": "ff104080-f0e4-4a2c-bc96-384f626522f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os.path\n",
        "import pickle\n",
        "import calendar\n",
        "\n",
        "# viz\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import vapeplot\n",
        "import arviz as az\n",
        "\n",
        "# modeling\n",
        "import pystan\n",
        "import stan_utility\n",
        "from sklearn.metrics import r2_score\n",
        "from project_utils.project_utils import *\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed5zssxrF6Eb"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VZB7OgpF6Ec"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use('seaborn')\n",
        "plt.rc('figure', figsize=(11.0, 7.0))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZtJblGFF6Eg"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lndHvFjsF6Eh"
      },
      "source": [
        "file_path = r'../../data/snow_data_clean.parquet'\n",
        "all_data_path = os.path.join(os.getcwd(), file_path)\n",
        "model_path = r'./stan_model.pkl'\n",
        "result_path = r'../../data/processed/stan_results.pkl'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLDY-_dF6Ek",
        "outputId": "ef135f16-f162-497c-9b16-d308a356821b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# parquet opening is broken on colab\n",
        "with open(file_path, 'rb') as parq_file:\n",
        "    long_series_df = pd.read_parquet(parq_file)\n",
        "assert long_series_df.base.isna().sum()==0\n",
        "\n",
        "long_series_df.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dayofyr</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>base</th>\n",
              "      <th>station</th>\n",
              "      <th>snowfall</th>\n",
              "      <th>ski_yr</th>\n",
              "      <th>state</th>\n",
              "      <th>region</th>\n",
              "      <th>pseudo_ts_delt</th>\n",
              "      <th>pseudo_ski_yr</th>\n",
              "      <th>pseudo_ts</th>\n",
              "      <th>basecol_interpolated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11085</th>\n",
              "      <td>137.0</td>\n",
              "      <td>2016-01-10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-10</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11086</th>\n",
              "      <td>138.0</td>\n",
              "      <td>2016-01-11</td>\n",
              "      <td>-2.320142</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-11</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11087</th>\n",
              "      <td>139.0</td>\n",
              "      <td>2016-01-12</td>\n",
              "      <td>-2.320142</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-12</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11088</th>\n",
              "      <td>140.0</td>\n",
              "      <td>2016-01-13</td>\n",
              "      <td>6.737995</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-13</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11089</th>\n",
              "      <td>141.0</td>\n",
              "      <td>2016-01-14</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-14</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       dayofyr  timestamp  ...  pseudo_ts basecol_interpolated\n",
              "11085    137.0 2016-01-10  ... 1692-01-10                 True\n",
              "11086    138.0 2016-01-11  ... 1692-01-11                 True\n",
              "11087    139.0 2016-01-12  ... 1692-01-12                 True\n",
              "11088    140.0 2016-01-13  ... 1692-01-13                False\n",
              "11089    141.0 2016-01-14  ... 1692-01-14                False\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfg37-k3F6En"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcKhhazNF6En"
      },
      "source": [
        "def add_month(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    return data.assign(month=lambda x:\n",
        "                       x.pseudo_ts.dt.month)\n",
        "\n",
        "def add_diff(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\" use difference in base, not absolute value \"\"\"\n",
        "    return (data\n",
        "            .assign(delta_base=lambda x: x.base.diff(1))\n",
        "            .fillna(0)\n",
        "            .drop(columns=['base'])\n",
        "           )\n",
        "\n",
        "def ohe(data: pd.DataFrame, col: str) -> pd.DataFrame:\n",
        "    return pd.concat([data.drop(columns=[col]),\n",
        "                      pd.get_dummies(data[col],\n",
        "                                     prefix=col)],\n",
        "                     axis=1)\n",
        "\n",
        "def add_month_x_snowfall(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"adds interaction terms\"\"\"\n",
        "    months = [col for col in data.columns\n",
        "              if 'month_' in col]\n",
        "    combos_df = pd.concat([pd.Series(data.snowfall * data[month],\n",
        "                                     name='snowfall_x_' + month)\n",
        "                           for month in months], axis=1)\n",
        "    return pd.concat([data, combos_df], axis=1)\n",
        "\n",
        "def cleaner(data: pd.DataFrame, includes: list=[None]) -> pd.DataFrame:\n",
        "    \"\"\" Removes interpolated rows and unneeded columns\n",
        "    Params:\n",
        "        data: df to operate on\n",
        "        includes: column names NOT to drop (don't need to specify usually)\n",
        "    ski_yr is needed for test/train split\"\"\"\n",
        "    data = data.query('basecol_interpolated==False')\n",
        "    bad_cols = ['dayofyr', 'station', 'state', 'pseudo_ski_yr',\n",
        "                'timestamp', 'basecol_interpolated', 'pseudo_ts',\n",
        "                'pseudo_ts_delt'\n",
        "               ]\n",
        "    bad_cols = [col for col in bad_cols if col not in includes]\n",
        "    return data.drop(columns=bad_cols)\n",
        "\n",
        "def sample_weighted_season(df: pd.DataFrame)->pd.DataFrame:\n",
        "    \"\"\"samples dataframe but doesn't remove rare months and mildly reduces\n",
        "    amount of semi-rare months\"\"\"\n",
        "    # un-OHE\n",
        "    df['month'] = df[[c for c in data.columns if \"month_\" in c and \"x_m\" not in c]].idxmax(axis=1)\n",
        "    # define months\n",
        "    rare_months = [f'month_{i}' for i in range(5,11)]\n",
        "    semirare_months = ['month_4', 'month_11']\n",
        "    nonrare_months = ['month_12', 'month_1', 'month_2', 'month_3']\n",
        "    # split and sample data\n",
        "    rare_data = df.query('month in @rare_months')\n",
        "    semirare_data = df.query('month in @semirare_months').sample(frac=.3, axis=0)\n",
        "    nonrare_data = df.query('month in @nonrare_months').sample(frac=.09, axis=0)\n",
        "    # recombine\n",
        "    return pd.concat([rare_data, semirare_data, nonrare_data], axis=0).drop(columns=['month'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xCn2bWK-bV5"
      },
      "source": [
        "### Split Data and sample dense areas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoUxdquzyI8V",
        "outputId": "f587b837-47e3-441c-9f25-ce0edb59296d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "source": [
        "# I want to oversample rare months, but will want to use the last year as the test set.\n",
        "long_series_df.query('basecol_interpolated==False')[['ski_yr', 'pseudo_ts', 'snowfall']]\\\n",
        ".assign(month=lambda x: x.pseudo_ts.dt.month)\\\n",
        ".drop(columns=['pseudo_ts'])\\\n",
        ".pivot_table(index=['month'], columns=['ski_yr'], values='snowfall', aggfunc='count')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>ski_yr</th>\n",
              "      <th>0.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>2.0</th>\n",
              "      <th>3.0</th>\n",
              "      <th>4.0</th>\n",
              "      <th>5.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>7.0</th>\n",
              "      <th>8.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4735.0</td>\n",
              "      <td>6067.0</td>\n",
              "      <td>6227.0</td>\n",
              "      <td>6240.0</td>\n",
              "      <td>6331.0</td>\n",
              "      <td>5975.0</td>\n",
              "      <td>5989.0</td>\n",
              "      <td>5914.0</td>\n",
              "      <td>5840.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4376.0</td>\n",
              "      <td>5545.0</td>\n",
              "      <td>5781.0</td>\n",
              "      <td>5716.0</td>\n",
              "      <td>5484.0</td>\n",
              "      <td>5650.0</td>\n",
              "      <td>5158.0</td>\n",
              "      <td>5277.0</td>\n",
              "      <td>4949.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4491.0</td>\n",
              "      <td>4810.0</td>\n",
              "      <td>5852.0</td>\n",
              "      <td>5795.0</td>\n",
              "      <td>5017.0</td>\n",
              "      <td>4507.0</td>\n",
              "      <td>4608.0</td>\n",
              "      <td>5060.0</td>\n",
              "      <td>5043.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1534.0</td>\n",
              "      <td>1596.0</td>\n",
              "      <td>1798.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>1318.0</td>\n",
              "      <td>1594.0</td>\n",
              "      <td>1646.0</td>\n",
              "      <td>1703.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>162.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>266.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>305.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>25.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>122.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NaN</td>\n",
              "      <td>54.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>735.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>1072.0</td>\n",
              "      <td>1060.0</td>\n",
              "      <td>1518.0</td>\n",
              "      <td>819.0</td>\n",
              "      <td>568.0</td>\n",
              "      <td>891.0</td>\n",
              "      <td>997.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4299.0</td>\n",
              "      <td>4471.0</td>\n",
              "      <td>4247.0</td>\n",
              "      <td>5293.0</td>\n",
              "      <td>5237.0</td>\n",
              "      <td>3367.0</td>\n",
              "      <td>4572.0</td>\n",
              "      <td>3976.0</td>\n",
              "      <td>4356.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "ski_yr     0.0     1.0     2.0     3.0     4.0     5.0     6.0     7.0     8.0\n",
              "month                                                                         \n",
              "1       4735.0  6067.0  6227.0  6240.0  6331.0  5975.0  5989.0  5914.0  5840.0\n",
              "2       4376.0  5545.0  5781.0  5716.0  5484.0  5650.0  5158.0  5277.0  4949.0\n",
              "3       4491.0  4810.0  5852.0  5795.0  5017.0  4507.0  4608.0  5060.0  5043.0\n",
              "4       1534.0  1596.0  1798.0  2009.0  1229.0  1318.0  1594.0  1646.0  1703.0\n",
              "5        162.0   153.0   210.0   266.0    92.0   183.0   178.0   185.0   305.0\n",
              "6         25.0    17.0    31.0    52.0    12.0    49.0    23.0    26.0   122.0\n",
              "7          2.0     2.0     4.0    27.0     NaN     1.0    17.0    10.0    31.0\n",
              "8          NaN     2.0     4.0     NaN     NaN     NaN     1.0     6.0     1.0\n",
              "9          NaN     NaN     NaN     1.0     NaN     NaN     NaN     1.0     1.0\n",
              "10         NaN    54.0    44.0    69.0    32.0    15.0    18.0    33.0    63.0\n",
              "11       735.0  1056.0  1072.0  1060.0  1518.0   819.0   568.0   891.0   997.0\n",
              "12      4299.0  4471.0  4247.0  5293.0  5237.0  3367.0  4572.0  3976.0  4356.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q11Mgqh69vMm"
      },
      "source": [
        "# set aside some data for multi-step analysis\n",
        "leaveouts = ['Eldora', 'Seven Springs']\n",
        "stan_multistep_test_df = (long_series_df.query('station in @leaveouts')\n",
        "                         .pipe(add_month)\n",
        "                         .pipe(add_diff)\n",
        "                         .pipe(ohe, 'region')\n",
        "                         .pipe(ohe, 'month')\n",
        "                         .pipe(cleaner))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__OGwbNgF6FR"
      },
      "source": [
        "stan_train_df, stan_test_df = (long_series_df\n",
        "           .drop(index=stan_multistep_test_df.index)\n",
        "           .pipe(add_month)\n",
        "           .pipe(add_diff)\n",
        "           .pipe(ohe, 'region')\n",
        "           .pipe(ohe, 'month')\n",
        "           .pipe(cleaner)\n",
        "           .pipe(train_test_split_ts, exog_cols='all', ski_yr_cutoff=7, as_monthly=False)\n",
        "           )\n",
        "stan_train_df = stan_train_df.pipe(sample_weighted_season)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL_pNkXf5Vob"
      },
      "source": [
        "# provide data including shapes and column type locations to stan\n",
        "columns = stan_df.columns\n",
        "region_cols = [c for c in columns if \"region\" in c]\n",
        "month_cols = [col for col in stan_sample_train_df.columns if \"month\" in col]\n",
        "\n",
        "def subsets(df: pd.DataFrame)-> tuple:\n",
        "    X = df.drop(columns=['delta_base'])\n",
        "    Xmonth= X[month_cols]\n",
        "    Xsnow = X['snowfall']\n",
        "    Xregion = X[region_cols]\n",
        "    y = df[['delta_base']]\n",
        "    return (X, Xmonth, Xsnow, Xregion, y)\n",
        "\n",
        "X, X_month, X_snow, X_region, y = subsets(stan_sample_train_df)\n",
        "X_test, X_month_test, X_snow_test, X_region_test, _ = subsets(stan_sample_test_df)\n",
        "\n",
        "stan_data = {'N': X.shape[0],\n",
        "             'K_month': X_month.shape[1],\n",
        "             'X_month': X_month.to_numpy(),\n",
        "             'K_reg': X_region.shape[1],\n",
        "             'X_reg': X_region.to_numpy(),\n",
        "             'X_snow': X_snow.to_numpy().reshape(-1,1),\n",
        "             'y': y.to_numpy().reshape(-1),\n",
        "             }\n",
        "stan_data_test = {**stan_data,\n",
        "                  'N_test': X_test.shape[0],\n",
        "                  'X_month_test': X_month_test.to_numpy(),\n",
        "                  'X_reg_test': X_region_test.to_numpy(),\n",
        "                  'X_snow_test': X_snow_test.to_numpy().reshape(-1,1),\n",
        "                  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m1rP0cIF6FR"
      },
      "source": [
        "# Bayesian Model in Stan (MCMC)\n",
        "I want to add priors to the model that snowfall should only result in increases in base depth, and monthly effects should only result in reduction (i.e., monthly effect should measure strength of melting.); changes at odds with this should be considered as noise. A bayesian model allows for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxc7NygqF6FU"
      },
      "source": [
        "functions_block = \"\"\"// for function\"\"\"\n",
        "data_block =  \"\"\"\n",
        "    // input data passed from Python\n",
        "    int<lower=1> N;               // number of data observations\n",
        "    int<lower=1> K_month;         // no of melting predictor\n",
        "    matrix[N, K_month] X_month;   // predictor for melting features\n",
        "    int<lower=1> K_reg;           // no of region features\n",
        "    matrix[N, K_reg] X_reg;       // region predictors\n",
        "    matrix[N, 1] X_snow;          // snowfall predictor\n",
        "    vector[N] y;                  // response vector\n",
        "    \n",
        "    // test variables\n",
        "    int<lower=1> N_test;                  // no of test records\n",
        "    matrix[N_test, K_month] X_month_test; // predictor for melting features\n",
        "    matrix[N_test, K_reg] X_reg_test;     // region predictors\n",
        "    matrix[N_test, 1] X_snow_test;\n",
        "    \"\"\"\n",
        "transformed_data_block = \"\"\"\n",
        "    matrix[N, K_reg] X_reg_snow;\n",
        "    row_vector[N] X_snow_rvect = to_row_vector(X_snow);\n",
        "    matrix[N_test, K_reg] X_reg_snow_test;\n",
        "    row_vector[N_test] X_snow_rvect_test = to_row_vector(X_snow_test);\n",
        "    \n",
        "    for (k in 1:K_reg) {          //  K_regxN * Nx1  T\n",
        "        for (n in 1:N) {\n",
        "            X_reg_snow[n,k] = X_snow_rvect[n] * X_reg[n,k];\n",
        "    }  }\n",
        "    \n",
        "    // same, but for test. Should do this with a function...\n",
        "    for (k in 1:K_reg) {\n",
        "        for (n in 1:N_test) {\n",
        "            X_reg_snow_test[n,k] = X_snow_rvect_test[n] * X_reg_test[n,k];\n",
        "    } }\"\"\"\n",
        "parameters_block = \"\"\"\n",
        "    // intercept was causing divergences and coef interpretation \n",
        "    // makes more sense without intercept: \n",
        "    // I don't expect change in base depth absent melting or snowfall\n",
        "    vector<upper=0>[K_month] beta_mo;           // coefficients for melting\n",
        "    vector<lower=0, upper=1>[K_reg] beta_reg_snow;       // coef for region x snow interaction\n",
        "    real<lower=0> sigma;                        // must be +ve\n",
        "    real<lower=0> sig_mos;                      // must be +ve\n",
        "    \"\"\"\n",
        "transformed_parameters_block = \"\"\"\"\"\"\n",
        "model_block = \"\"\"\n",
        "    vector[N] mu;                       // y_hat\n",
        "    sigma ~ cauchy(0, 10);              // half Cauchy\n",
        "    sig_mos ~ cauchy(0, 20);\n",
        "    for (n in 1:K_month) {\n",
        "        beta_mo[n] ~ normal(0, sig_mos) T[,0]; // sample from normal, only -ve\n",
        "    }\n",
        "    // prior on snow columns is beta over [0,1]\n",
        "    beta_reg_snow ~ beta(2.2, 3);         // reparameterize so this and snow are from beta dist\n",
        "    mu = X_month*beta_mo + X_reg_snow*beta_reg_snow;\n",
        "    y ~ normal(mu, sigma);\n",
        "    \"\"\"\n",
        "generated_quantities_block = \"\"\"\n",
        "    vector[N_test] y_test;\n",
        "    for(n in 1:N_test) {\n",
        "        y_test[n] = normal_rng(X_month_test[n]*beta_mo + \n",
        "                               X_reg_snow_test[n]*beta_reg_snow, sigma);\n",
        "    \"\"\"\n",
        "# assemble model\n",
        "def create_stan_model(functions_block=functions_block, data_block=data_block, \n",
        "                      transformed_data_block=transformed_data_block, \n",
        "                      parameters_block=parameters_block, \n",
        "                      transformed_parameters_block=transformed_parameters_block, model_block=model_block,\n",
        "                      generated_quantities_block=generated_quantities_block):\n",
        "    return f'''\n",
        "    functions {{{functions_block}}}\n",
        "    data {{{data_block}}}\n",
        "    transformed data {{{transformed_data_block}}}\n",
        "    parameters {{{parameters_block}}}\n",
        "    transformed parameters {{{transformed_parameters_block}}}\n",
        "    model {{{model_block}}}\n",
        "    generated quantities {{{generated_quantities_block}}}'''\n",
        "\n",
        "stan_model_str = create_stan_model(generated_quantities_block=\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffQAVs5kt23C"
      },
      "source": [
        "sm = pystan.StanModel(model_code=stan_model_str, model_name='stan_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT3bX-l5ocXx"
      },
      "source": [
        "# avoid recompile if possible\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(sm, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_ZZ4pquEND"
      },
      "source": [
        "fit = sm.sampling(data=stan_data, iter=2_000, chains=4, n_jobs=-1,\n",
        "                  sample_file=\"../../data/processed/stan_samples\",\n",
        "                  control={'adapt_delta': 0.85, # p accepting posterior draw\n",
        "                           'stepsize': 1,  # just starting stepsize\n",
        "                          }, \n",
        "                  seed=42, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJXVG5wGhUeI"
      },
      "source": [
        "# for overnight run\n",
        "try:\n",
        "    with open(result_path, 'wb') as f:\n",
        "        pickle.dump(fit, f)\n",
        "# reload saved objects if not reruning sampler\n",
        "except NameError:\n",
        "    with open(model_path, 'rb') as f:\n",
        "        sm = pickle.load(f)\n",
        "    with open(result_path, 'rb') as f:\n",
        "        fit = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWB3_b-Fx7Dp"
      },
      "source": [
        "## MCMC Diagnostics\n",
        "We will want to check:\n",
        "1. Model actually runs.\n",
        "1. Good Mixing of Chains: (fix with stronger prior, reparameterization)\n",
        "    1. $\\hat{R}$ is 1.1 or under for all parameters.\n",
        "    1. When n_eff / n_transitions < 0.001 the estimators that we use are often biased and can significantly overestimate the true effective sample size.\n",
        "1. Check tree depth:\n",
        "if threshold saturated, increase tree depth _control={max_treedepth: 15}_\n",
        "1. \n",
        "\n",
        "_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSPHbBtmns3M"
      },
      "source": [
        "stan_utility.check_all_diagnostics(fit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiBiPS4n-Boz"
      },
      "source": [
        "## Visualization of results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-i-s8UKnjA8"
      },
      "source": [
        "fit_az = az.from_pystan(posterior=fit,\n",
        "                        dims={'beta_reg_snow': ['Coefficients_for_Snow_by_Region'],\n",
        "                              'beta_mo': ['Melting_Coefficients_by_Month']},\n",
        "                        coords={'Coefficients_for_Snow_by_Region': X_region.columns.values.tolist(),\n",
        "                                'Melting_Coefficients_by_Month': [calendar.month_name[i+1] for i in range(12)]}\n",
        "                        )\n",
        "rc = {'plot.max_subplots': None}\n",
        "az.rcParams.update(rc)\n",
        "sns.set_style('whitegrid')\n",
        "az.plot_trace(fit_az)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtAQQbXgoJM4"
      },
      "source": [
        "# fix brackets in col nmaes\n",
        "fit_df = (fit.to_dataframe()\n",
        "          .rename(columns=lambda x: x.replace(\"[\", \"_\"))\n",
        "          .rename(columns=lambda x: x.replace(\"]\", \"\")))\n",
        "\n",
        "fit_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnInGaRAaRfY"
      },
      "source": [
        "# get region names without \"region_\"\n",
        "region_names = [reg[7:] for reg in X_region.columns.values.tolist()]\n",
        "\n",
        "region_betas_df = fit_df.filter(regex=\"reg\", axis=1)\n",
        "reg_cols = region_betas_df.columns\n",
        "region_betas_df = (region_betas_df\n",
        "                   .rename(columns={col: reg_name for col, reg_name \n",
        "                                    in zip(reg_cols, region_names)})\n",
        "                   .melt(var_name=\"region\"))\n",
        "region_betas_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE97qf4OZxiU"
      },
      "source": [
        "plt.style.use('bmh')\n",
        "fig = sns.kdeplot(x=region_betas_df.value, hue=region_betas_df.region, fill=True, cut=0, bw_adjust=.3)\n",
        "plt.suptitle(\"Estimated Base Increase per Unit of Snowfall\", fontsize=20)\n",
        "plt.xlabel(\"Effect of Unit of Powder\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2D_363pltuR"
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "jazzcup = sns.blend_palette(vapeplot.palette(\"jazzcup\"), n_colors=region_betas_df.region.unique().size)\n",
        "f, ax = plt.subplots(figsize=(12, 8))\n",
        "sort_order = region_betas_df.groupby(['region']).mean().sort_values(by='value', ascending=True).index\n",
        "\n",
        "sns.violinplot(x='region', y='value', data=region_betas_df,\n",
        "            order=sort_order, palette=jazzcup)\n",
        "\n",
        "plt.title(\"Estimated Base Increase per Unit of Snowfall: Bayesian Model\", fontsize=20)\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Fraction of Full Unit of Powder');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twx31uBSsgyN"
      },
      "source": [
        "month_betas_df = fit_df.filter(like='beta_mo').melt(var_name=\"month\")\n",
        "month_betas_df = month_betas_df[month_betas_df.value > month_betas_df.value.quantile(.02)]\n",
        "month_map = {f\"beta_mo_{i}\": calendar.month_abbr[i] for i in range(1, 13)}\n",
        "#month_betas_df['month'] = pd.to_datetime(month_betas_df['month'].replace(month_map), format=\"%B\").dt.month.astype('category')\n",
        "month_betas_df['month'] = month_betas_df['month'].replace(month_map).astype('str')\n",
        "month_betas_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXKPQVlt_gB"
      },
      "source": [
        "def plot_snow_betas(df, start_mo):\n",
        "    fig, ax = plt.subplots()\n",
        "    month_ordered = [mo for mo in calendar.month_abbr[1:] if mo in df.month.unique()]\n",
        "    start_mo_ix = month_ordered.index(start_mo)\n",
        "    month_ordered = month_ordered[start_mo_ix:] + month_ordered[:start_mo_ix]\n",
        "    sns.boxplot(data=df, y='value', x='month', order=month_ordered,\n",
        "                ax=ax, )\n",
        "    ax.set_ylabel('Inches Melted per Day')\n",
        "    ax.set_xlabel('Month')\n",
        "    ax.set_title('Estimated Snow Melted per Day by Month');\n",
        "plot_snow_betas(month_betas_df, \"Jan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPhSA24RJfJD"
      },
      "source": [
        "These estimates are mostly expected, but there seems to be low melting amounts during summer...this can be explained when we realize that most of the values for May-November were interpolated. The averages aren't weighted by ski acreage, so the large number of small ski stations on the east coast & midwest with short seasons are disproportionately affecting these numbers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blPU-QZ4HHce"
      },
      "source": [
        "interpo_ratios=(long_series_df\n",
        "    .assign(month=lambda x: x.pseudo_ts.dt.month)\n",
        "    .groupby('month')\n",
        "    .apply(lambda x: x.basecol_interpolated.sum()/x.shape[0])\n",
        "    .to_frame()\n",
        "    .reset_index()\n",
        "    .rename(columns={0:'ratio'})\n",
        ")\n",
        "fig, ax = plt.subplots()\n",
        "sns.barplot(data=interpo_ratios, x='month', y='ratio', ax=ax)\n",
        "plt.title('Fraction of Base observations that were interpolated', fontsize=15)\n",
        "[plt.text((i-.17), value+.01, str(value)) for i, value in enumerate(interpo_ratios.ratio.round(2).to_numpy())]\n",
        "months_xticks(ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhsp01-EJze7"
      },
      "source": [
        "plot_snow_betas(month_betas_df[~month_betas_df.month.isin(['Jun', 'Jul', 'Aug', 'Sep', 'Oct'])], \"Nov\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LatVl9EK8ELg"
      },
      "source": [
        "### Check test set metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tjQVAuawYuu"
      },
      "source": [
        "stan_model_wtest_str = create_stan_model()\n",
        "sm_wtest = pystan.StanModel(model_code=stan_model_str, model_name='stan_model')\n",
        "fit_wtest = sm.sampling(data=stan_data_test, iter=2_000, chains=4, n_jobs=-1,\n",
        "                  sample_file=\"../../data/processed/stan_samples\",\n",
        "                  control={'adapt_delta': 0.85, # p accepting posterior draw\n",
        "                           'stepsize': 1,  # just starting stepsize\n",
        "                          }, \n",
        "                  seed=42, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7-L230X7ses"
      },
      "source": [
        "y_pred = pd.DataFrame(data=fit_wtest.extract(['y_test'], inc_warmup=False)['y_test'].T.mean(axis=1),\n",
        "                      columns=['y_pred'])\n",
        "test_results = pd.concat([y_test.reset_index(drop=True), y_pred], axis=1)\n",
        "\n",
        "r2_score(y_true=test_results.delta_base, y_pred=test_results.y_pred, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67BUD9Du-z_D"
      },
      "source": [
        "# Aggregate by month and see if r2 is affected\n",
        "y_pred_month_agg = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXz3lIHLbnqQ"
      },
      "source": [
        "# Stan model including ARMA terms\n",
        "I will not be using a seasonal model, since I think using months as predictors is more useful to capture seasonal changes in an interpretable way. However, I do want to caputure ARMA terms to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u054FjPQ_Bxx",
        "outputId": "11c433e9-8606-4ca3-c17b-6f0d09712fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "AC_PAC_plotter(df=long_series_df.query('ski_yr==1'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b10933f4b2a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAC_PAC_plotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlong_series_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ski_yr==1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AC_PAC_plotter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzuITDh9b4Hh"
      },
      "source": [
        "create_stan_model(model_block=)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}