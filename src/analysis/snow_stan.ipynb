{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snow_nonts_analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCGOsNC1F6EV"
      },
      "source": [
        "# Description\n",
        "This will focus on using classic regression models and bayesian models. \n",
        "\n",
        "#### NOTE: As described in EDA notebook, \"Pseudo_ts\" is concatenation of data from locally adjacent ski resorts (e.g., all resorts in Colorado) into a single timeseries.\n",
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hkRrNSaGDae",
        "outputId": "309147c1-a228-4718-e332-d6cd07c4169a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "! pip install vapeplot pystan arviz stan_utility"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vapeplot in /usr/local/lib/python3.6/dist-packages (0.0.8)\n",
            "Requirement already satisfied: pystan in /usr/local/lib/python3.6/dist-packages (2.19.1.1)\n",
            "Requirement already satisfied: arviz in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: stan_utility in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from vapeplot) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from vapeplot) (1.18.5)\n",
            "Requirement already satisfied: Cython!=0.25.1,>=0.22 in /usr/local/lib/python3.6/dist-packages (from pystan) (0.29.21)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.6/dist-packages (from arviz) (50.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from arviz) (20.4)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from arviz) (1.4.1)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.6/dist-packages (from arviz) (1.5.4)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from arviz) (1.1.2)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from arviz) (0.16.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from stan_utility) (2.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->vapeplot) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->arviz) (1.15.0)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.6/dist-packages (from netcdf4->arviz) (1.2.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->arviz) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNlQEkolNSWR",
        "outputId": "17551999-4d6a-4337-922b-4bf5c6488a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    base_path = r'/content/gdrive/My Drive/data_sci/colab/ski/'\n",
        "    os.chdir(base_path)\n",
        "    try:\n",
        "        ! git clone https://github.com/chrisoyer/ski-snow-modeling/\n",
        "    except:  # if dir not empty e.g. already cloned\n",
        "        pass\n",
        "    mod_path = os.path.join(base_path, \n",
        "                            r\"ski-snow-modeling/src/analysis/project_utils/project_utils.py\")\n",
        "    import importlib.util\n",
        "    spec = importlib.util.spec_from_file_location(name=\"utils.name\", location=mod_path)\n",
        "    utils = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(utils)\n",
        "    \n",
        "    os.chdir('./ski-snow-modeling/src/analysis/')\n",
        "    # Change the working directory to the repo root.\n",
        "    # Add the repo root to the Python path.\n",
        "    import sys\n",
        "    sys.path.append(os.getcwd())\n",
        "else:\n",
        "    # local running\n",
        "    import project_utils as utils"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "fatal: destination path 'ski-snow-modeling' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7TktoEhF6EX"
      },
      "source": [
        "# data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os.path\n",
        "import pickle\n",
        "import calendar\n",
        "\n",
        "# viz\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import vapeplot\n",
        "import arviz as az\n",
        "\n",
        "# modeling\n",
        "import pystan\n",
        "import stan_utility\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed5zssxrF6Eb"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VZB7OgpF6Ec"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use('seaborn')\n",
        "plt.rc('figure', figsize=(11.0, 7.0))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZtJblGFF6Eg"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lndHvFjsF6Eh"
      },
      "source": [
        "file_path = r'../../data/snow_data_clean.parquet'\n",
        "all_data_path = os.path.join(os.getcwd(), file_path)\n",
        "model_path = r'./stan_model.pkl'\n",
        "result_path = r'../../data/processed/stan_results.pkl'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLDY-_dF6Ek",
        "outputId": "667cd564-a92a-4a70-d6ba-2ea4f5a3a5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# parquet opening is broken on colab\n",
        "with open(file_path, 'rb') as parq_file:\n",
        "    long_series_df = pd.read_parquet(parq_file)\n",
        "assert long_series_df.base.isna().sum()==0\n",
        "\n",
        "long_series_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dayofyr</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>base</th>\n",
              "      <th>station</th>\n",
              "      <th>snowfall</th>\n",
              "      <th>ski_yr</th>\n",
              "      <th>state</th>\n",
              "      <th>region</th>\n",
              "      <th>pseudo_ts_delt</th>\n",
              "      <th>pseudo_ski_yr</th>\n",
              "      <th>pseudo_ts</th>\n",
              "      <th>basecol_interpolated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11085</th>\n",
              "      <td>137.0</td>\n",
              "      <td>2016-01-10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-10</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11086</th>\n",
              "      <td>138.0</td>\n",
              "      <td>2016-01-11</td>\n",
              "      <td>-2.320142</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-11</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11087</th>\n",
              "      <td>139.0</td>\n",
              "      <td>2016-01-12</td>\n",
              "      <td>-2.320142</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-12</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11088</th>\n",
              "      <td>140.0</td>\n",
              "      <td>2016-01-13</td>\n",
              "      <td>6.737995</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-13</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11089</th>\n",
              "      <td>141.0</td>\n",
              "      <td>2016-01-14</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>Mt. Holiday</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>michigan</td>\n",
              "      <td>Other</td>\n",
              "      <td>324.0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>1692-01-14</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       dayofyr  timestamp  ...  pseudo_ts basecol_interpolated\n",
              "11085    137.0 2016-01-10  ... 1692-01-10                 True\n",
              "11086    138.0 2016-01-11  ... 1692-01-11                 True\n",
              "11087    139.0 2016-01-12  ... 1692-01-12                 True\n",
              "11088    140.0 2016-01-13  ... 1692-01-13                False\n",
              "11089    141.0 2016-01-14  ... 1692-01-14                False\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfg37-k3F6En"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcKhhazNF6En"
      },
      "source": [
        "def add_month(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    return data.assign(month=lambda x:\n",
        "                       x.pseudo_ts.dt.month)\n",
        "\n",
        "def add_diff(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\" use difference in base, not absolute value \"\"\"\n",
        "    return (data\n",
        "            .assign(delta_base=lambda x: x.base.diff(1))\n",
        "            .fillna(0)\n",
        "            .drop(columns=['base'])\n",
        "           )\n",
        "\n",
        "def ohe(data: pd.DataFrame, col: str) -> pd.DataFrame:\n",
        "    return pd.concat([data.drop(columns=[col]),\n",
        "                      pd.get_dummies(data[col],\n",
        "                                     prefix=col)],\n",
        "                     axis=1)\n",
        "\n",
        "def add_month_x_snowfall(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"adds interaction terms\"\"\"\n",
        "    months = [col for col in data.columns\n",
        "              if 'month_' in col]\n",
        "    combos_df = pd.concat([pd.Series(data.snowfall * data[month],\n",
        "                                     name='snowfall_x_' + month)\n",
        "                           for month in months], axis=1)\n",
        "    return pd.concat([data, combos_df], axis=1)\n",
        "\n",
        "def cleaner(data: pd.DataFrame, includes: list=[None]) -> pd.DataFrame:\n",
        "    \"\"\" Removes interpolated rows and unneeded columns\n",
        "    Params:\n",
        "        data: df to operate on\n",
        "        includes: column names NOT to drop (don't need to specify usually)\n",
        "    ski_yr is needed for test/train split\"\"\"\n",
        "    data = data.query('basecol_interpolated==False')\n",
        "    bad_cols = ['dayofyr', 'station', 'state', 'pseudo_ski_yr',\n",
        "                'timestamp', 'basecol_interpolated', 'pseudo_ts',\n",
        "                'pseudo_ts_delt'\n",
        "               ]\n",
        "    bad_cols = [col for col in bad_cols if col not in includes]\n",
        "    return data.drop(columns=bad_cols)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI5Yw01cF6Er",
        "outputId": "4b0a6a64-652d-445f-e967-b03b2bdb74d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "data = (long_series_df.pipe(add_month)\n",
        "        .pipe(add_diff)\n",
        "        .pipe(ohe, 'month')\n",
        "        .pipe(add_month_x_snowfall)\n",
        "        .pipe(cleaner)\n",
        ")\n",
        "data.sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "snowfall                                                          239805\n",
              "ski_yr                                                            847944\n",
              "region                 OtherOtherOtherOtherOtherOtherOtherOtherOtherO...\n",
              "delta_base                                                       57308.9\n",
              "month_1                                                            53318\n",
              "month_2                                                            47936\n",
              "month_3                                                            45183\n",
              "month_4                                                            14427\n",
              "month_5                                                             1734\n",
              "month_6                                                              357\n",
              "month_7                                                               94\n",
              "month_8                                                               14\n",
              "month_9                                                                3\n",
              "month_10                                                             328\n",
              "month_11                                                            8716\n",
              "month_12                                                           39818\n",
              "snowfall_x_month_1                                                 54429\n",
              "snowfall_x_month_2                                                 61551\n",
              "snowfall_x_month_3                                                 47591\n",
              "snowfall_x_month_4                                                 13534\n",
              "snowfall_x_month_5                                                   982\n",
              "snowfall_x_month_6                                                    33\n",
              "snowfall_x_month_7                                                     0\n",
              "snowfall_x_month_8                                                     0\n",
              "snowfall_x_month_9                                                     0\n",
              "snowfall_x_month_10                                                  242\n",
              "snowfall_x_month_11                                                 9598\n",
              "snowfall_x_month_12                                                51845\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m1rP0cIF6FR"
      },
      "source": [
        "# Bayesian Model in Stan (MCMC)\n",
        "I want to add priors to the model that snowfall should only result in increases in base depth, and monthly effects should only result in reduction (i.e., monthly effect should measure strength of melting.); changes at odds with this should be considered as noise. A bayesian model allows for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__OGwbNgF6FR"
      },
      "source": [
        "stan_df = (long_series_df\n",
        "           .pipe(add_month)\n",
        "           .pipe(add_diff)\n",
        "           .pipe(ohe, 'region')\n",
        "           .pipe(ohe, 'month')\n",
        "           .pipe(cleaner)\n",
        "           )\n",
        "stan_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_7mt-C1Vi60"
      },
      "source": [
        "# sample data; half million records => slow mcmc\n",
        "#stan_sample_full_df = (stan_df.sample(frac=.16, axis=0, replace=False))\n",
        "\n",
        "def sample_weighted_season(df: pd.DataFrame)->pd.DataFrame:\n",
        "    \"\"\"samples dataframe but doesn't remove rare months and mildly reduces\n",
        "    amount of semi-rare months\"\"\"\n",
        "    # un-OHE\n",
        "    df['month'] = df[[c for c in data.columns if \"month_\" in c and \"x_m\" not in c]].idxmax(axis=1)\n",
        "    # define months\n",
        "    rare_months = [f'month_{i}' for i in range(5,11)]\n",
        "    semirare_months = ['month_4', 'month_11']\n",
        "    nonrare_months = ['month_12', 'month_1', 'month_2', 'month_3']\n",
        "    # split and sample data\n",
        "    rare_data = df.query('month in @rare_months')\n",
        "    semirare_data = df.query('month in @semirare_months').sample(frac=.3, axis=0)\n",
        "    nonrare_data = df.query('month in @nonrare_months').sample(frac=.09, axis=0)\n",
        "    # recombine\n",
        "    return pd.concat([rare_data, semirare_data, nonrare_data], axis=0).drop(columns=['month'])\n",
        "\n",
        "# split data first so rare months get included in both sets\n",
        "stan_sample_test_df = stan_df.sample(frac=.20, axis=0)\n",
        "stan_sample_train_df = stan_df.drop(index=stan_sample_test_df.index)\n",
        "# reduce data size \n",
        "stan_sample_test_df = stan_df.pipe(sample_weighted_season)\n",
        "stan_sample_train_df = stan_df.pipe(sample_weighted_season)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL_pNkXf5Vob"
      },
      "source": [
        "# provide data including shapes and column type locations to stan\n",
        "columns = stan_df.columns\n",
        "region_cols = [c for c in columns if \"region\" in c]\n",
        "month_cols = [col for col in stan_sample_train_df.columns if \"month\" in col]\n",
        "\n",
        "X = stan_sample_train_df.drop(columns=['delta_base'])\n",
        "X_month= X[month_cols]\n",
        "X_snow = X['snowfall']\n",
        "X_region = X[region_cols]\n",
        "y = stan_sample_train_df[['delta_base']]\n",
        "\n",
        "X_test = stan_sample_test_df.drop(columns=['delta_base'])\n",
        "X_month_test = X_test[month_cols]\n",
        "X_snow_test = X_test['snowfall']\n",
        "X_region_test = X_test[region_cols]\n",
        "y_test = stan_sample_test_df[['delta_base']]\n",
        "\n",
        "stan_data = {'N': X.shape[0],\n",
        "             'K_month': X_month.shape[1],\n",
        "             'X_month': X_month.to_numpy(),\n",
        "             'K_reg': X_region.shape[1],\n",
        "             'X_reg': X_region.to_numpy(),\n",
        "             'X_snow': X_snow.to_numpy().reshape(-1,1),\n",
        "             'y': y.to_numpy().reshape(-1),\n",
        "             # test\n",
        "             'N_test': X_test.shape[0],\n",
        "             'X_month_test': X_month_test.to_numpy(),\n",
        "             'X_reg_test': X_region_test.to_numpy(),\n",
        "             'X_snow_test': X_snow_test.to_numpy().reshape(-1,1),\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxc7NygqF6FU"
      },
      "source": [
        "stan_model_str = \"\"\"\n",
        "functions {}\n",
        "data {\n",
        "    // input data passed from Python\n",
        "    int<lower=1> N;               // number of data observations\n",
        "    int<lower=1> K_month;         // no of melting predictor\n",
        "    matrix[N, K_month] X_month;   // predictor for melting features\n",
        "    int<lower=1> K_reg;           // no of region features\n",
        "    matrix[N, K_reg] X_reg;       // region predictors\n",
        "    matrix[N, 1] X_snow;          // snowfall predictor\n",
        "    vector[N] y;                  // response vector\n",
        "    \n",
        "    // test variables\n",
        "    int<lower=1> N_test;                  // no of test records\n",
        "    matrix[N_test, K_month] X_month_test; // predictor for melting features\n",
        "    matrix[N_test, K_reg] X_reg_test;     // region predictors\n",
        "    matrix[N_test, 1] X_snow_test;\n",
        "}\n",
        "transformed data {\n",
        "    matrix[N, K_reg] X_reg_snow;\n",
        "    row_vector[N] X_snow_rvect = to_row_vector(X_snow);\n",
        "    matrix[N_test, K_reg] X_reg_snow_test;\n",
        "    row_vector[N_test] X_snow_rvect_test = to_row_vector(X_snow_test);\n",
        "    \n",
        "    for (k in 1:K_reg) {          //  K_regxN * Nx1  T\n",
        "        for (n in 1:N) {\n",
        "            X_reg_snow[n,k] = X_snow_rvect[n] * X_reg[n,k];\n",
        "    }  }\n",
        "    \n",
        "    // same, but for test. Should do this with a function...\n",
        "    for (k in 1:K_reg) {\n",
        "        for (n in 1:N_test) {\n",
        "            X_reg_snow_test[n,k] = X_snow_rvect_test[n] * X_reg_test[n,k];\n",
        "    }  }\n",
        "}\n",
        "parameters {\n",
        "    // intercept was causing divergences and coef interpretation \n",
        "    // makes more sense without intercept: \n",
        "    // I don't expect change in base depth absent melting or snowfall\n",
        "    vector<upper=0>[K_month] beta_mo;           // coefficients for melting\n",
        "    vector<lower=0, upper=1>[K_reg] beta_reg_snow;       // coef for region x snow interaction\n",
        "    real<lower=0> sigma;                        // must be +ve\n",
        "    real<lower=0> sig_mos;                      // must be +ve\n",
        "}\n",
        "transformed parameters {\n",
        "    vector[N] mu;                       // y_hat\n",
        "    mu = X_month*beta_mo + X_reg_snow*beta_reg_snow;\n",
        "}\n",
        "model {\n",
        "    sigma ~ cauchy(0, 10);              // half Cauchy\n",
        "    sig_mos ~ cauchy(0, 20);\n",
        "    for (n in 1:K_month) {\n",
        "        beta_mo[n] ~ normal(0, sig_mos) T[,0]; // sample from normal, only -ve\n",
        "    }\n",
        "    // prior on snow columns is beta over [0,1]\n",
        "    beta_reg_snow ~ beta(2.2, 3);         // reparameterize so this and snow are from beta dist\n",
        "    y ~ normal(mu, sigma);\n",
        "}\n",
        "generated quantities{\n",
        "    vector[N_test] y_test;\n",
        "    for(n in 1:N_test) {\n",
        "        y_test[n] = normal_rng(X_month_test[n]*beta_mo + \n",
        "                               X_reg_snow_test[n]*beta_reg_snow, sigma);\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffQAVs5kt23C"
      },
      "source": [
        "sm = pystan.StanModel(model_code=stan_model_str, model_name='stan_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT3bX-l5ocXx"
      },
      "source": [
        "# avoid recompile if possible\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(sm, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_ZZ4pquEND"
      },
      "source": [
        "fit = sm.sampling(data=stan_data, iter=2_000, chains=4, n_jobs=-1,\n",
        "                  sample_file=\"../../data/processed/stan_samples\"\n",
        "                  control={'adapt_delta': 0.80, # p accepting posterior draw\n",
        "                           'stepsize': .1,}, \n",
        "                  seed=42, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKRmIvzkD4hq"
      },
      "source": [
        "fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJXVG5wGhUeI"
      },
      "source": [
        "# for overnight run\n",
        "try:\n",
        "    dir(fit)\n",
        "except NameError:\n",
        "    with open(model_path, 'rb') as f:\n",
        "        sm = pickle.load(f)\n",
        "    with open(result_path, 'rb') as f:\n",
        "        fit = pickle.load(f)\n",
        "else:\n",
        "    with open(result_path, 'wb') as f:\n",
        "        pickle.dump(fit, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWB3_b-Fx7Dp"
      },
      "source": [
        "## MCMC Diagnostics\n",
        "We will want to check:\n",
        "1. Model actually runs.\n",
        "1. Good Mixing of Chains: (fix with stronger prior, reparameterization)\n",
        "    1. $\\hat{R}$ is 1.1 or under for all parameters.\n",
        "    1. When n_eff / n_transitions < 0.001 the estimators that we use are often biased and can significantly overestimate the true effective sample size.\n",
        "1. Check tree depth:\n",
        "if threshold saturated, increase tree depth _control={max_treedepth: 15}_\n",
        "1. \n",
        "\n",
        "_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSPHbBtmns3M"
      },
      "source": [
        "stan_utility.check_all_diagnostics(fit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtAQQbXgoJM4"
      },
      "source": [
        "# fix brackets in col nmaes\n",
        "fit_df = (fit.to_dataframe()\n",
        "          .rename(columns=lambda x: x.replace(\"[\", \"_\"))\n",
        "          .rename(columns=lambda x: x.replace(\"]\", \"\")))\n",
        "\n",
        "fit_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiBiPS4n-Boz"
      },
      "source": [
        "## Visualization of results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-i-s8UKnjA8"
      },
      "source": [
        "fit_az = az.from_pystan(posterior=fit,\n",
        "                        dims={'beta_reg_snow': ['Coefficients_for_Snow_by_Region'],\n",
        "                              'beta_mo': ['Melting_Coefficients_by_Month']},\n",
        "                        coords={'Coefficients_for_Snow_by_Region': X_region.columns.values.tolist(),\n",
        "                                'Melting_Coefficients_by_Month': [calendar.month_name[i+1] for i in range(12)]}\n",
        "                        )\n",
        "rc = {'plot.max_subplots': None}\n",
        "az.rcParams.update(rc)\n",
        "sns.set_style('whitegrid')\n",
        "az.plot_trace(fit_az)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnInGaRAaRfY"
      },
      "source": [
        "# get region names without \"region_\"\n",
        "region_names = [reg[7:] for reg in X_region.columns.values.tolist()]\n",
        "\n",
        "region_betas_df = fit_df.filter(regex=\"reg\", axis=1)\n",
        "reg_cols = region_betas_df.columns\n",
        "region_betas_df = (region_betas_df\n",
        "                   .rename(columns={col: reg_name for col, reg_name \n",
        "                                    in zip(reg_cols, region_names)})\n",
        "                   .melt(var_name=\"region\"))\n",
        "region_betas_df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE97qf4OZxiU"
      },
      "source": [
        "plt.style.use('bmh')\n",
        "fig = sns.kdeplot(x=region_betas_df.value, hue=region_betas_df.region, fill=True, cut=0, bw_adjust=.3)\n",
        "plt.suptitle(\"Estimated Base Increase per Unit of Snowfall\", fontsize=20)\n",
        "plt.xlabel(\"Effect of Unit of Powder\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2D_363pltuR"
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "jazzcup = sns.blend_palette(vapeplot.palette(\"jazzcup\"), n_colors=region_betas_df.region.unique().size)\n",
        "f, ax = plt.subplots(figsize=(12, 8))\n",
        "sort_order = region_betas_df.groupby(['region']).mean().sort_values(by='value', ascending=True).index\n",
        "\n",
        "sns.violinplot(x='region', y='value', data=region_betas_df,\n",
        "            order=sort_order, palette=jazzcup)\n",
        "\n",
        "plt.title(\"Estimated Base Increase per Unit of Snowfall: Bayesian Model\", fontsize=20)\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Fraction of Full Unit of Powder');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twx31uBSsgyN"
      },
      "source": [
        "month_betas_df = fit_df.filter(like='beta_mo').melt(var_name=\"month\")\n",
        "month_betas_df = month_betas_df[month_betas_df.value > month_betas_df.value.quantile(.02)]\n",
        "month_map = {f\"beta_mo_{i}\": calendar.month_abbr[i] for i in range(1, 13)}\n",
        "#month_betas_df['month'] = pd.to_datetime(month_betas_df['month'].replace(month_map), format=\"%B\").dt.month.astype('category')\n",
        "month_betas_df['month'] = month_betas_df['month'].replace(month_map).astype('str')\n",
        "month_betas_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXKPQVlt_gB"
      },
      "source": [
        "def plot_snow_betas(df, start_mo):\n",
        "    fig, ax = plt.subplots()\n",
        "    month_ordered = [mo for mo in calendar.month_abbr[1:] if mo in df.month.unique()]\n",
        "    start_mo_ix = month_ordered.index(start_mo)\n",
        "    month_ordered = month_ordered[start_mo_ix:] + month_ordered[:start_mo_ix]\n",
        "    sns.boxplot(data=df, y='value', x='month', order=month_ordered,\n",
        "                ax=ax, )\n",
        "    ax.set_ylabel('Inches Melted per Day')\n",
        "    ax.set_xlabel('Month')\n",
        "    ax.set_title('Estimated Snow Melted per Day by Month');\n",
        "plot_snow_betas(month_betas_df, \"Jan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPhSA24RJfJD"
      },
      "source": [
        "These estimates are mostly expected, but there seems to be low melting amounts during summer...this can be explained when we realize that most of the values for May-November were interpolated. The averages aren't weighted by ski acreage, so the large number of small ski stations on the east coast & midwest with short seasons are disproportionately affecting these numbers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blPU-QZ4HHce"
      },
      "source": [
        "interpo_ratios=(long_series_df\n",
        "    .assign(month=lambda x: x.pseudo_ts.dt.month)\n",
        "    .groupby('month')\n",
        "    .apply(lambda x: x.basecol_interpolated.sum()/x.shape[0])\n",
        "    .to_frame()\n",
        "    .reset_index()\n",
        "    .rename(columns={0:'ratio'})\n",
        ")\n",
        "fig, ax = plt.subplots()\n",
        "sns.barplot(data=interpo_ratios, x='month', y='ratio', ax=ax)\n",
        "plt.title('Fraction of Base observations that were interpolated', fontsize=15)\n",
        "[plt.text((i-.17), value+.01, str(value)) for i, value in enumerate(interpo_ratios.ratio.round(2).to_numpy())]\n",
        "months_xticks(ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhsp01-EJze7"
      },
      "source": [
        "plot_snow_betas(month_betas_df[~month_betas_df.month.isin(['Jun', 'Jul', 'Aug', 'Sep', 'Oct'])], \"Nov\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJgH7yRoKqRv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}